直接逐行往下执行很难 精准的往下一层 进入，

那么我们可以先把后端的关键入口函数 搞清楚，在入口打上断点，往上走：

后端文件的分工如下：

硬件后端抽象层。
ggml-backend-impl.h:    定义了所有硬件后端（CPU, CUDA, Metal等）必须遵守的接口规范。
ggml-backend.cpp:       实现了调用具体后端接口的通用逻辑和计算任务调度器，能将一个计算图拆分到多个硬件上执行。
ggml-backend-reg.cpp:   负责管理、注册和发现可用的硬件后端，支持静态编译和动态加载。

我们现搞清楚 上面这几个文件的 组织方式 ，就可以准确的打点了。

ggml.h / ggml.c / ggml.cpp:                 ggml 核心库。定义了张量（tensor）、计算图（cgraph）等基本数据结构，以及在其上执行的各种数学运算（如矩阵乘法、加法等）。
ggml-alloc.c:                               内存分配器。为计算图中的张量提供高效的内存管理策略，包括一个动态分配器，用于在计算过程中复用内存。

ggml-quants.c / ggml-quants.h:              量化函数库。包含了将浮点张量转换为各种低精度（如4位、8位）量化格式以及反量化的核心算法。
ggml-threading.cpp / ggml-threading.h:      线程工具。提供简单的跨平台线程同步功能（互斥锁），以保证库在多线程环境下的安全。
gguf.cpp:                                   GGUF 文件格式处理器。负责读取和写入 GGUF 格式的模型文件，解析模型的元数据和张量数据。
ggml-opt.cpp:                               优化与训练框架。在 ggml 基础上实现了模型训练所需的功能，包括数据集处理、损失函数计算、反向传播和优化器（如 AdamW）的应用。


大致的后端 架构 清楚了， 下一步和 cann 实际后端 对接上，  看看  alc 的具体初始化， 缓存管理  流管理， 究竟几个流在执行，每个流在干啥。


然后就需要理解debug 一次 ，启动起来，   这里 已经成功了


然后要搞清楚  kv-cache 究竟是怎么 实现的 什么 机制  
    更加细节的 调度机制。 


最后就可以 搞清楚 Tensor 的具体的计算的过程了


=============================

到目前为止我们基本搞清了 llama.cpp 层定义了main函数入口，model数据结构，llama_context是整个runtime核心，启动llama.cpp 主要就是启动llama_context
我们还了解了ggml 是先在几定义了一套 ggml_tensor 以及它的算子，然后定义graph 以及调度它的scheduler, 然后tensor要存在真实的内存上，算子要运行在真实的计算卡上，所以需要对应的后端来执行。
为此用编译注册和接口隔离的方式支持了多款后端，这些后端的启动涉及 类型发现，设备发现，设备注册，设备后端实例化，设备后端实例控制设备启动，分配内存，等等一整套初始化。
ggml层定义的算子，对应的在每个后端都有一个实现，cgraph会被发给后端执行，对应的后端会遍历每个节点，执行该节点上的算子，然后这个算子都有一个对应的后端上的实现，真正在设备上计算。

ggml_cann_backend 实现了 又ggml_backend 定义的 启动接口，申请 buffer 的接口， 执行计算图的接口 
ggml 中实现并提供了 创建tensor  计算tensor 创建buffer 创建cgraph  创建scheduler  执行计算图等接口
llama.cpp 中 定义了model能转为cgraph  设计了llama_context运行时  设计了mem管理   设计kv-cache 串起来就可以LLm推理了

但是我们还是有大片的迷雾：
    1. kv-cache 究竟是怎么运行来加速和节省内存的      完成了
          为了实现该kv-cache机制，下一层的mem接口又是怎么设计的  搞定了
    2. ggml中的tensor 以及基础算子的计算能否有一个更加清晰透彻的理解   这个解决了
    3. GGUF 这种文件的设计究竟是怎么实现的，和pth相比有何优势        
    4. 开始研究llama.cpp中主打的量化应该如何实操， 量化的通识，量化在llama.cpp中的实现    

    

============ todo ==================================

// 从磁盘加载模型权重文件，返回 llama_model 对象，这个函数主要解决了 模型的加载流程和权重映射
static struct llama_model * llama_model_load_from_file_impl(
        const std::string & path_model,
        std::vector<std::string> & splits,
        struct llama_model_params params)  




